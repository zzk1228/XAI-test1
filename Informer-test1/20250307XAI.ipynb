{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 消化informer时间的处理方式，使其不再依赖年月日"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(model='informer', data='ECL', root_path='./data/', data_path='ECL.csv', features='M', target='MT_320', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, enc_in=321, dec_in=321, c_out=321, d_model=512, n_heads=8, e_layers=2, d_layers=1, s_layers=[3, 2, 1], d_ff=2048, factor=5, padding=0, distil=True, dropout=0.05, attn='prob', embed='timeF', activation='gelu', output_attention=False, do_predict=False, mix=True, cols=None, num_workers=0, itr=2, train_epochs=6, batch_size=32, patience=3, learning_rate=0.0001, des='test', loss='mse', lradj='type1', use_amp=False, inverse=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', do_interpret=True, detail_freq='h')\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informer_ECL_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18293\n",
      "val 2609\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3876532\n",
      "\tspeed: 0.0895s/iter; left time: 297.9105s\n",
      "\titers: 200, epoch: 1 | loss: 0.2389333\n",
      "\tspeed: 0.0686s/iter; left time: 221.3986s\n",
      "\titers: 300, epoch: 1 | loss: 0.2199720\n",
      "\tspeed: 0.0688s/iter; left time: 215.1794s\n",
      "\titers: 400, epoch: 1 | loss: 0.1873639\n",
      "\tspeed: 0.0694s/iter; left time: 210.0125s\n",
      "\titers: 500, epoch: 1 | loss: 0.1622280\n",
      "\tspeed: 0.0701s/iter; left time: 205.3079s\n",
      "Epoch: 1 cost time: 40.67884588241577\n",
      "Epoch: 1, Steps: 571 | Train Loss: 0.3007652 Vali Loss: 0.2121881 Test Loss: 0.2763771\n",
      "Validation loss decreased (inf --> 0.212188).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1432178\n",
      "\tspeed: 0.1951s/iter; left time: 537.6582s\n",
      "\titers: 200, epoch: 2 | loss: 0.1481788\n",
      "\tspeed: 0.0731s/iter; left time: 194.0274s\n",
      "\titers: 300, epoch: 2 | loss: 0.1429945\n",
      "\tspeed: 0.0730s/iter; left time: 186.6075s\n",
      "\titers: 400, epoch: 2 | loss: 0.1286816\n",
      "\tspeed: 0.0692s/iter; left time: 170.0483s\n",
      "\titers: 500, epoch: 2 | loss: 0.1393284\n",
      "\tspeed: 0.0704s/iter; left time: 165.9424s\n",
      "Epoch: 2 cost time: 40.611489057540894\n",
      "Epoch: 2, Steps: 571 | Train Loss: 0.1463739 Vali Loss: 0.1887825 Test Loss: 0.2476114\n",
      "Validation loss decreased (0.212188 --> 0.188782).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1241454\n",
      "\tspeed: 0.1936s/iter; left time: 422.9626s\n",
      "\titers: 200, epoch: 3 | loss: 0.1202958\n",
      "\tspeed: 0.0710s/iter; left time: 148.0051s\n",
      "\titers: 300, epoch: 3 | loss: 0.1317169\n",
      "\tspeed: 0.0713s/iter; left time: 141.4801s\n",
      "\titers: 400, epoch: 3 | loss: 0.1148183\n",
      "\tspeed: 0.0710s/iter; left time: 133.7755s\n",
      "\titers: 500, epoch: 3 | loss: 0.1144373\n",
      "\tspeed: 0.0723s/iter; left time: 129.0163s\n",
      "Epoch: 3 cost time: 40.73060750961304\n",
      "Epoch: 3, Steps: 571 | Train Loss: 0.1231599 Vali Loss: 0.1788993 Test Loss: 0.2413196\n",
      "Validation loss decreased (0.188782 --> 0.178899).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1122352\n",
      "\tspeed: 0.1947s/iter; left time: 314.3178s\n",
      "\titers: 200, epoch: 4 | loss: 0.1144627\n",
      "\tspeed: 0.0709s/iter; left time: 107.3158s\n",
      "\titers: 300, epoch: 4 | loss: 0.1101997\n",
      "\tspeed: 0.0709s/iter; left time: 100.1904s\n",
      "\titers: 400, epoch: 4 | loss: 0.1153163\n",
      "\tspeed: 0.0711s/iter; left time: 93.4880s\n",
      "\titers: 500, epoch: 4 | loss: 0.1129592\n",
      "\tspeed: 0.0721s/iter; left time: 87.5161s\n",
      "Epoch: 4 cost time: 40.71355891227722\n",
      "Epoch: 4, Steps: 571 | Train Loss: 0.1144048 Vali Loss: 0.1740993 Test Loss: 0.2346126\n",
      "Validation loss decreased (0.178899 --> 0.174099).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1188353\n",
      "\tspeed: 0.1956s/iter; left time: 203.9700s\n",
      "\titers: 200, epoch: 5 | loss: 0.1142473\n",
      "\tspeed: 0.0722s/iter; left time: 68.1087s\n",
      "\titers: 300, epoch: 5 | loss: 0.1059683\n",
      "\tspeed: 0.0726s/iter; left time: 61.2125s\n",
      "\titers: 400, epoch: 5 | loss: 0.1109154\n",
      "\tspeed: 0.0741s/iter; left time: 55.0214s\n",
      "\titers: 500, epoch: 5 | loss: 0.1166633\n",
      "\tspeed: 0.0728s/iter; left time: 46.8067s\n",
      "Epoch: 5 cost time: 41.60728120803833\n",
      "Epoch: 5, Steps: 571 | Train Loss: 0.1103817 Vali Loss: 0.1760093 Test Loss: 0.2349113\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1087326\n",
      "\tspeed: 0.2062s/iter; left time: 97.3369s\n",
      "\titers: 200, epoch: 6 | loss: 0.1061029\n",
      "\tspeed: 0.0737s/iter; left time: 27.4168s\n",
      "\titers: 300, epoch: 6 | loss: 0.1051259\n",
      "\tspeed: 0.0746s/iter; left time: 20.3022s\n",
      "\titers: 400, epoch: 6 | loss: 0.1158087\n",
      "\tspeed: 0.0732s/iter; left time: 12.5868s\n",
      "\titers: 500, epoch: 6 | loss: 0.1068453\n",
      "\tspeed: 0.0742s/iter; left time: 5.3426s\n",
      "Epoch: 6 cost time: 42.23247694969177\n",
      "Epoch: 6, Steps: 571 | Train Loss: 0.1083577 Vali Loss: 0.1761134 Test Loss: 0.2344024\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      ">>>>>>>testing : informer_ECL_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (163, 32, 24, 321) (163, 32, 24, 321)\n",
      "test shape: (5216, 24, 321) (5216, 24, 321)\n",
      "mse:0.23470191657543182, mae:0.34186506271362305\n",
      ">>>>>>>interpretability analysis : informer_ECL_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ZhangJiaKun\\trans\\Informer-test1\\main_informer.py\", line 115, in <module>\n",
      "    interpret_path = exp.interpret(setting, True)\n",
      "  File \"c:\\Users\\ZhangJiaKun\\trans\\Informer-test1\\exp\\exp_informer.py\", line 335, in interpret\n",
      "    self._analyze_temporal_patterns(interpreter, sample_batch, interpret_path)\n",
      "  File \"c:\\Users\\ZhangJiaKun\\trans\\Informer-test1\\exp\\exp_informer.py\", line 402, in _analyze_temporal_patterns\n",
      "    fig = interpreter.visualize_temporal_patterns(batch_x, batch_x_mark, batch_y, batch_y_mark, feature_idx)\n",
      "  File \"c:\\Users\\ZhangJiaKun\\trans\\Informer-test1\\utils\\interpret.py\", line 289, in visualize_temporal_patterns\n",
      "    avg_attn = np.mean(avg_attn_across_layers, axis=0)\n",
      "  File \"c:\\Users\\ZhangJiaKun\\miniconda3\\envs\\transformers\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 3504, in mean\n",
      "    return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "  File \"c:\\Users\\ZhangJiaKun\\miniconda3\\envs\\transformers\\lib\\site-packages\\numpy\\core\\_methods.py\", line 102, in _mean\n",
      "    arr = asanyarray(a)\n",
      "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n"
     ]
    }
   ],
   "source": [
    "!python main_informer.py --model informer --data ECL --features M --seq_len 96 --label_len 48 --pred_len 24 --e_layers 2 --d_layers 1 --attn prob --do_interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
